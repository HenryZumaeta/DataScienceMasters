{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryZumaeta/MDS_UNI/blob/Zeta/CICLO02/DL/C06_20240515_Guardado_y_carga_de_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxjpzKTvg_dd"
      },
      "source": [
        "# Guardar y cargar modelos\n",
        "\n",
        "En este tutorial aprenderemos cómo podemos tomar un modelo entrenado, guardarlo y luego volver a cargarlo para seguir entrenándolo o usarlo para realizar inferencias. En particular, utilizaremos el aprendizaje por transferencia para entrenar un clasificador para clasificar imágenes de perros y gatos, tal como lo hicimos en la lección anterior. Luego tomaremos nuestro modelo entrenado y lo guardaremos como un archivo HDF5, que es el formato utilizado por Keras. Luego cargaremos este modelo, lo usaremos para realizar predicciones y luego continuaremos entrenando el modelo. Finalmente, guardaremos nuestro modelo entrenado como TensorFlow SavedModel y luego lo descargaremos a un disco local, para que luego pueda usarse para su implementación en diferentes plataformas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crU-iluJIEzw"
      },
      "source": [
        "## Conceptos que se cubrirán en este Colab\n",
        "\n",
        "1. Guardar modelos en formato HDF5 para Keras\n",
        "2. Guardar modelos en el formato TensorFlow SavedModel\n",
        "3. Cargando modelos\n",
        "4. Descargar modelos al disco local\n",
        "\n",
        "Antes de iniciar este Colab, debe restablecer el entorno de Colab seleccionando `Runtime -> Reset all runtimes...` en el menú de arriba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RVsYZLEpEWs"
      },
      "source": [
        "# Importaciones\n",
        "\n",
        "En este Colab usaremos la versión Beta de TensorFlow 2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3BXzUGabcI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55147b3b-1fd0-4cde-9125-6f170f6d7881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.2.2)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.66.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.5.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.11.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow_datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow_hub\n",
        " # Instala o actualiza la biblioteca TensorFlow Hub para cargar modelos preentrenados desde TensorFlow Hub.\n",
        "\n",
        "!pip install -U tensorflow_datasets\n",
        " # Instala o actualiza la biblioteca TensorFlow Datasets para acceder a conjuntos de datos estándar para tareas de aprendizaje automático.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28UtPNlizGI"
      },
      "source": [
        "Algunas importaciones normales que hemos visto antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGNpmn43C0O6"
      },
      "outputs": [],
      "source": [
        "import time  # Importa el módulo 'time' para trabajar con funciones relacionadas con el tiempo.\n",
        "import numpy as np  # Importa NumPy, una biblioteca para trabajar con arreglos y operaciones matemáticas.\n",
        "import matplotlib.pylab as plt  # Importa Matplotlib, una biblioteca para crear gráficos y visualizaciones.\n",
        "\n",
        "import tensorflow as tf\n",
        "# Importa TensorFlow, una biblioteca popular para el aprendizaje profundo y otras tareas de aprendizaje automático.\n",
        "import tensorflow_hub as hub\n",
        "# Importa TensorFlow Hub para acceder a modelos preentrenados y sus características.\n",
        "import tensorflow_datasets as tfds\n",
        " # Importa TensorFlow Datasets para acceder a conjuntos de datos estándar y facilitar su uso en el aprendizaje automático.\n",
        "tfds.disable_progress_bar()  # Desactiva la barra de progreso de TensorFlow Datasets para una salida más limpia en la consola.\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "# Importa el módulo 'layers' de la biblioteca Keras de TensorFlow para crear capas de redes neuronales y otros componentes del modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amfzqn1Oo7Om"
      },
      "source": [
        "# Parte 1: cargar el conjunto de datos de gatos y perros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93vvAdGxDMD"
      },
      "source": [
        "Usaremos conjuntos de datos de TensorFlow para cargar el conjunto de datos de Perros vs Gatos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrIUV3V0xDL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8b68fb-b22e-41aa-c5e7-9a4437320ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "(train_examples, validation_examples), info = tfds.load(\n",
        "    'cats_vs_dogs',  # Carga el conjunto de datos 'cats_vs_dogs' desde TensorFlow Datasets.\n",
        "    split=['train[:80%]', 'train[80%:]'],  # Divide el conjunto de entrenamiento en 80% para entrenamiento y 20% para validación.\n",
        "    with_info=True,  # Incluye información adicional sobre el conjunto de datos, como el número de ejemplos y características.\n",
        "    as_supervised=True,  # Carga el conjunto de datos en un formato (imagen, etiqueta) para facilitar el uso en tareas supervisadas.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbgpD3E6gM2P"
      },
      "source": [
        "Las imágenes del conjunto de datos Perros vs. Gatos no son todas del mismo tamaño. Entonces, necesitamos reformatear todas las imágenes a la resolución esperada por MobileNet (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we_ftzQxNf7e"
      },
      "outputs": [],
      "source": [
        "def format_image(image, label):\n",
        "    # Los módulos de imagen de 'hub' esperan que los datos estén normalizados en el rango [0, 1].\n",
        "    image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / 255.0  # Normaliza la imagen al rango [0, 1] y redimensiona a IMAGE_RES x IMAGE_RES.\n",
        "    return image, label\n",
        "\n",
        "num_examples = info.splits['train'].num_examples  # Obtiene el número total de ejemplos en el conjunto de entrenamiento.\n",
        "\n",
        "BATCH_SIZE = 32  # Establece el tamaño del lote para el entrenamiento y la validación.\n",
        "IMAGE_RES = 224  # Establece la resolución de las imágenes a 224x224 píxeles para el formato utilizado en el modelo.\n",
        "\n",
        "train_batches = train_examples.cache().shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "# Prepara lotes de entrenamiento: almacena en caché, mezcla, aplica la función de formato de imagen, crea lotes y prefetch para la eficiencia.\n",
        "\n",
        "validation_batches = validation_examples.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "# Prepara lotes de validación: almacena en caché, aplica la función de formato de imagen, crea lotes y prefetch para la eficiencia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzV457OXreQP"
      },
      "source": [
        "# Parte 2: Transferir el aprendizaje con TensorFlow Hub\n",
        "\n",
        "Ahora usaremos TensorFlow Hub para realizar Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wB030nezBwI"
      },
      "outputs": [],
      "source": [
        "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "feature_extractor = hub.KerasLayer(URL,\n",
        "                                   input_shape=(IMAGE_RES, IMAGE_RES,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFmF7A5E4tk"
      },
      "source": [
        "Congele las variables en la capa del extractor de características, de modo que el entrenamiento solo modifique la capa del clasificador final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg5ar6rcE4H-"
      },
      "outputs": [],
      "source": [
        "feature_extractor.trainable = False\n",
        "# Establece la capa de extracción de características (MobileNetV2) como no entrenable.\n",
        "# Al fijar esta capa como no entrenable, sus pesos\n",
        "# y parámetros no se actualizarán durante el entrenamiento del modelo, lo que preserva las características aprendidas previamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPVeouTksO9q"
      },
      "source": [
        "## Adjunte un encabezado de clasificación\n",
        "\n",
        "Ahora envuelva la capa central en un modelo `tf.keras.Sequential` y agregue una nueva capa de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGcY27fY1q3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272c8e13-fa71-4673-8d83-da366d8ae90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2260546 (8.62 MB)\n",
            "Trainable params: 2562 (10.01 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,  # Agrega la capa de extracción de características (MobileNetV2) al modelo secuencial.\n",
        "    layers.Dense(2)  # Agrega una capa densa con 2 unidades para la clasificación binaria (en este caso, 2 clases: gato y perro).\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "# Muestra un resumen del modelo, incluyendo la arquitectura de las capas,\n",
        "# el número de parámetros entrenables y no entrenables, y la forma de salida de cada capa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHbXQqIquFxQ"
      },
      "source": [
        "## Entrena el modelo\n",
        "\n",
        "Ahora entrenamos este modelo como cualquier otro, llamando primero a `compile` y luego a `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n0Wb9ylKd8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02cf582-d20d-47ca-f9d9-4d237841e8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "582/582 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9773"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',  # Configura el optimizador Adam para el entrenamiento del modelo.\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # Utiliza la entropía cruzada categórica como función de pérdida para problemas de clasificación.\n",
        "    metrics=['accuracy']  # Utiliza la precisión como métrica para evaluar el rendimiento del modelo durante el entrenamiento.\n",
        ")\n",
        "\n",
        "EPOCHS = 1  # Establece el número de épocas para entrenar el modelo.\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,  # Utiliza el lote de entrenamiento para entrenar el modelo.\n",
        "    epochs=EPOCHS,  # Número de épocas para entrenar el modelo.\n",
        "    validation_data=validation_batches\n",
        "    # Utiliza el lote de validación para evaluar el modelo después de cada época de entrenamiento.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb__ZN8uFn-D"
      },
      "source": [
        "## Consulta las predicciones\n",
        "\n",
        "Obtenga la lista ordenada de nombres de clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_Zvg2i0fzJu"
      },
      "outputs": [],
      "source": [
        "class_names = np.array(info.features['label'].names)\n",
        "# Obtiene los nombres de las clases del conjunto de datos y los almacena en un array NumPy llamado 'class_names'.\n",
        "\n",
        "print(class_names)\n",
        "# Imprime los nombres de las clases para mostrar las etiquetas de las clases disponibles en el conjunto de datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Olg6MsNGJTL"
      },
      "source": [
        "Ejecute un lote de imágenes a través del modelo y convierta los índices en nombres de clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCLVCpEjJ_VP"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
        "# Obtiene un lote de imágenes y sus etiquetas del conjunto de entrenamiento utilizando la función 'iter' y 'take'.\n",
        "\n",
        "image_batch = image_batch.numpy()\n",
        "label_batch = label_batch.numpy()\n",
        "# Convierte el lote de imágenes y etiquetas de TensorFlow a arrays NumPy para facilitar el procesamiento.\n",
        "\n",
        "predicted_batch = model.predict(image_batch)\n",
        "# Realiza predicciones utilizando el modelo en el lote de imágenes obtenido.\n",
        "\n",
        "predicted_batch = tf.squeeze(predicted_batch).numpy()\n",
        "# Elimina dimensiones adicionales de las predicciones utilizando 'tf.squeeze' y convierte a un array NumPy.\n",
        "\n",
        "predicted_ids = np.argmax(predicted_batch, axis=-1)\n",
        "# Obtiene los índices de las clases predichas con mayor probabilidad para cada imagen.\n",
        "\n",
        "predicted_class_names = class_names[predicted_ids]\n",
        "# Obtiene los nombres de las clases predichas utilizando los índices y los nombres de las clases disponibles en 'class_names'.\n",
        "\n",
        "print(predicted_class_names)\n",
        "# Imprime los nombres de las clases predichas para el lote de imágenes dado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGbZxl9GZs-"
      },
      "source": [
        "Veamos las etiquetas verdaderas y las previstas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9IhOmGI5dJ"
      },
      "outputs": [],
      "source": [
        "print(\"Labels: \", label_batch)\n",
        "# Imprime las etiquetas reales del lote de imágenes, mostrando las clases verdaderas correspondientes a cada imagen.\n",
        "\n",
        "print(\"Predicted labels: \", predicted_ids)\n",
        "# Imprime las clases predichas para cada imagen en el lote, mostrando los índices de las clases predichas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC_AYRJU9NQe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 9))\n",
        "for n in range(30):\n",
        "    plt.subplot(6, 5, n+1)\n",
        "    plt.imshow(image_batch[n])  # Muestra la imagen en la posición 'n' del lote.\n",
        "\n",
        "    color = \"blue\" if predicted_ids[n] == label_batch[n] else \"red\"\n",
        "    # Establece el color del título como azul si la predicción es correcta y rojo si es incorrecta.\n",
        "\n",
        "    plt.title(predicted_class_names[n].title(), color=color)\n",
        "    # Muestra el nombre de la clase predicha en el título de la imagen, con la primera letra en mayúscula.\n",
        "\n",
        "    plt.axis('off')  # Desactiva los ejes para mostrar solo las imágenes sin marco.\n",
        "\n",
        "_ = plt.suptitle(\"Model predictions (blue: correct, red: incorrect)\")\n",
        "# Muestra un título global que indica la interpretación del color en las predicciones (azul: correcto, rojo: incorrecto).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPQYQLx3cYq"
      },
      "source": [
        "# Parte 3: Guardar como modelo Keras `.h5`\n",
        "\n",
        "Ahora que hemos entrenado el modelo, podemos guardarlo como un archivo HDF5, que es el formato utilizado por Keras. Nuestro archivo HDF5 tendrá la extensión '.h5' y su nombre corresponderá a la marca de tiempo actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCnNWTkZ3Ckz"
      },
      "outputs": [],
      "source": [
        "t = time.time()  # Obtiene el tiempo actual en segundos.\n",
        "\n",
        "export_path_keras = \"./{}.h5\".format(int(t))  # Crea un nombre de archivo para el modelo exportado en formato h5.\n",
        "print(export_path_keras)  # Imprime la ruta del archivo donde se guardará el modelo en formato h5.\n",
        "\n",
        "model.save(export_path_keras)  # Guarda el modelo en el archivo especificado en formato h5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tdJWVHmnKxJ"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgqmH1WVUKli"
      },
      "source": [
        "Posteriormente podrá recrear el mismo modelo a partir de este archivo, incluso si ya no tiene acceso al código que creó el modelo.\n",
        "\n",
        "Este archivo incluye:\n",
        "\n",
        "- La arquitectura del modelo.\n",
        "- Los valores de peso del modelo (que se aprendieron durante el entrenamiento)\n",
        "- La configuración de entrenamiento del modelo (lo que pasaste a \"compilar\"), si corresponde.\n",
        "- El optimizador y su estado, si lo hubiera (esto le permite reiniciar el entrenamiento donde lo dejó)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXole25Z799G"
      },
      "source": [
        "# Parte 4: Cargar el modelo `.h5` de Keras\n",
        "\n",
        "Ahora cargaremos el modelo que acabamos de guardar en un nuevo modelo llamado \"recargado\". Necesitaremos proporcionar la ruta del archivo y el parámetro `custom_objects`. Este parámetro le dice a Keras cómo cargar `hub.KerasLayer` desde `feature_extractor` que usamos para el aprendizaje por transferencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx-z3Qwx5RnB"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.keras.models.load_model(\n",
        "    export_path_keras,\n",
        "    # Carga el modelo desde el archivo especificado en 'export_path_keras'.\n",
        "\n",
        "    custom_objects={'KerasLayer': hub.KerasLayer}\n",
        "    # Especifica cómo cargar la capa 'hub.KerasLayer' del modelo, ya que es una capa personalizada utilizada en la red neuronal.\n",
        ")\n",
        "\n",
        "reloaded.summary()\n",
        "# Muestra un resumen del modelo cargado, incluyendo la arquitectura de las capas,\n",
        "#el número de parámetros entrenables y no entrenables, y la forma de salida de cada capa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxDl2vPkf2ST"
      },
      "source": [
        "Podemos comprobar que el modelo recargado y el modelo anterior dan el mismo resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFljA-Hd85Tu"
      },
      "outputs": [],
      "source": [
        "result_batch = model.predict(image_batch)\n",
        "# Realiza predicciones en el lote de imágenes 'image_batch' utilizando el modelo original ('model').\n",
        "\n",
        "reloaded_result_batch = reloaded.predict(image_batch)\n",
        "# Realiza predicciones en el mismo lote de imágenes 'image_batch' utilizando el modelo recargado ('reloaded').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeCZUUJK9Svv"
      },
      "source": [
        "La diferencia en la producción debe ser cero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3p5-uD39PC1"
      },
      "outputs": [],
      "source": [
        "(abs(result_batch - reloaded_result_batch)).max()\n",
        "# Calcula el valor absoluto de la diferencia entre los resultados predichos por el modelo original y el modelo recargado para cada elemento del lote.\n",
        "# Luego, encuentra el valor máximo de estas diferencias.\n",
        "# Esta expresión evalúa la máxima discrepancia entre las predicciones del modelo original y el modelo recargado en el conjunto de imágenes proporcionado.\n",
        "# Un valor cercano a cero indica que las predicciones son prácticamente idénticas entre los dos modelos para las imágenes dadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqU79kKFo7S2"
      },
      "source": [
        "Como podemos ver, el resultado es 0,0, lo que indica que ambos modelos hicieron las mismas predicciones en el mismo lote de imágenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKunO4soA4Dm"
      },
      "source": [
        "# Sigue entrenando\n",
        "\n",
        "Además de hacer predicciones, también podemos tomar nuestro modelo `reloaded` y seguir entrenándolo. Para hacer esto, puedes entrenar el `reloaded` como de costumbre, usando el método `.fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEv-fnHEAplx"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3  # Número de épocas para entrenar el modelo recargado ('reloaded').\n",
        "\n",
        "history = reloaded.fit(\n",
        "    train_batches,  # Utiliza el lote de entrenamiento para entrenar el modelo.\n",
        "    epochs=EPOCHS,  # Número de épocas para entrenar el modelo recargado.\n",
        "    validation_data=validation_batches  # Utiliza el lote de validación para evaluar el modelo después de cada época de entrenamiento.\n",
        ")\n",
        "# Entrena el modelo recargado durante el número especificado de épocas utilizando los lotes de entrenamiento y validación.\n",
        "# El progreso del entrenamiento y las métricas se almacenan en el objeto 'history' para su posterior análisis y visualización.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OKoZaAHFH_s"
      },
      "source": [
        "# Parte 5: Exportar como modelo guardado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V47IwQbFTYT"
      },
      "source": [
        "También puede exportar un modelo completo al formato TensorFlow SavedModel. SavedModel es un formato de serialización independiente para objetos de Tensorflow, compatible con el servicio TensorFlow, así como con implementaciones de TensorFlow distintas de Python. Un SavedModel contiene un programa TensorFlow completo, que incluye pesos y cálculos. No requiere la ejecución del código de creación del modelo original, lo que lo hace útil para compartir o implementar (con TFLite, TensorFlow.js, TensorFlow Serving o TFHub).\n",
        "\n",
        "Los archivos SavedModel que se crearon contienen:\n",
        "\n",
        "* Un punto de control de TensorFlow que contiene los pesos del modelo.\n",
        "* Un prototipo de SavedModel que contiene el gráfico subyacente de Tensorflow. Se guardan gráficos separados para predicción (publicación), entrenamiento y evaluación. Si el modelo no se compiló antes, solo se exporta el gráfico de inferencia.\n",
        "* La configuración de arquitectura del modelo, si está disponible.\n",
        "\n",
        "\n",
        "Guardemos nuestro `model` original como TensorFlow SavedModel. Para hacer esto usaremos la función `tf.saved_model.save()`. Esta función toma el modelo que queremos guardar y la ruta a la carpeta donde queremos guardar nuestro modelo.\n",
        "\n",
        "Esta función creará una carpeta donde encontrará una carpeta `assets`, una carpeta `variables` y el archivo `saved_model.pb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtpeKMfoGXrj"
      },
      "outputs": [],
      "source": [
        "t = time.time()  # Obtiene el tiempo actual en segundos.\n",
        "\n",
        "export_path_sm = \"./{}\".format(int(t))  # Crea un nombre de carpeta para el modelo exportado.\n",
        "print(export_path_sm)  # Imprime la ruta de la carpeta donde se guardarán los archivos del modelo.\n",
        "\n",
        "tf.saved_model.save(model, export_path_sm)\n",
        "# Guarda el modelo utilizando el formato SavedModel en la carpeta especificada en 'export_path_sm'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h6B1wITlu-9"
      },
      "outputs": [],
      "source": [
        "!ls {export_path_sm}\n",
        "# Utiliza el comando 'ls' para listar los archivos y directorios dentro de la carpeta especificada por 'export_path_sm'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktpsqHxJPIQW"
      },
      "source": [
        "# Parte 6: Cargar modelo guardado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0FDcCn9ncDb"
      },
      "source": [
        "Ahora, carguemos nuestro SavedModel y usémoslo para hacer predicciones. Usamos la función `tf.saved_model.load()` para cargar nuestros SavedModels. El objeto devuelto por `tf.saved_model.load` es 100% independiente del código que lo creó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7_PM7lofG2V"
      },
      "outputs": [],
      "source": [
        "reloaded_sm = tf.saved_model.load(export_path_sm)\n",
        "# Carga el modelo SavedModel desde la carpeta especificada en 'export_path_sm' utilizando la función 'tf.saved_model.load()'.\n",
        "\n",
        "# Después de cargar el modelo SavedModel, el modelo reloaded_sm está listo para ser utilizado para la inferencia y otras operaciones de TensorFlow.\n",
        "# Este paso es crucial para cargar el modelo guardado y poder usarlo en aplicaciones y servicios en producción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esEODdtM0kHB"
      },
      "source": [
        "Ahora, usemos `reloaded_sm` (Reloaded SavedModel) para hacer predicciones en un lote de imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpQldy_bm3Ty"
      },
      "outputs": [],
      "source": [
        "reload_sm_result_batch = reloaded_sm(image_batch, training=False).numpy()\n",
        "# Utiliza el modelo SavedModel ('reloaded_sm') para realizar inferencias en el lote de imágenes 'image_batch'.\n",
        "# La opción 'training=False' indica que el modelo se utiliza en modo de inferencia, no en modo de entrenamiento.\n",
        "\n",
        "# La función devuelve un array NumPy que contiene las predicciones del modelo SavedModel en el conjunto de imágenes dado.\n",
        "# Estas predicciones se almacenan en la variable 'reload_sm_result_batch' para su posterior análisis o visualización.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65upCyVN0u3Q"
      },
      "source": [
        "Podemos comprobar que el SavedModel recargado y el modelo anterior dan el mismo resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoCwmkGznR_0"
      },
      "outputs": [],
      "source": [
        "(abs(result_batch - reload_sm_result_batch)).max()\n",
        "# Calcula el valor absoluto de la diferencia entre los resultados predichos por el modelo original y el modelo SavedModel para cada elemento del lote.\n",
        "# Luego, encuentra el valor máximo de estas diferencias.\n",
        "\n",
        "# Esta expresión evalúa la máxima discrepancia entre las predicciones del modelo original y el modelo SavedModel en el conjunto de imágenes proporcionado.\n",
        "# Un valor cercano a cero indica que las predicciones son prácticamente idénticas entre los dos modelos para las imágenes dadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxuETjjs01pL"
      },
      "source": [
        "Como podemos ver, el resultado es 0,0, lo que indica que ambos modelos hicieron las mismas predicciones en el mismo lote de imágenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Nom-ka0yuqB"
      },
      "source": [
        "# Parte 7: Carga del modelo guardado como modelo de Keras\n",
        "\n",
        "El objeto devuelto por `tf.saved_model.load` no es un objeto Keras (es decir, no tiene los métodos `.fit`, `.predict`, `.summary`, etc.). Por lo tanto, no puedes simplemente tomar tu modelo `reloaded_sm` y seguir entrenándolo ejecutando `.fit`. Para poder recuperar un modelo keras completo desde el formato Tensorflow SavedModel debemos usar la función `tf.keras.models.load_model`. Esta función funcionará igual que antes, excepto que ahora pasamos la ruta a la carpeta que contiene nuestro SavedModel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi1jaqh8yvt6"
      },
      "outputs": [],
      "source": [
        "t = time.time()  # Obtiene el tiempo actual en segundos.\n",
        "\n",
        "export_path_sm = \"./{}\".format(int(t))  # Crea un nombre de carpeta para el modelo exportado.\n",
        "print(export_path_sm)  # Imprime la ruta de la carpeta donde se guardarán los archivos del modelo.\n",
        "\n",
        "tf.saved_model.save(model, export_path_sm)\n",
        "# Guarda el modelo utilizando el formato SavedModel en la carpeta especificada en 'export_path_sm'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VPE2_QQGmAP"
      },
      "outputs": [],
      "source": [
        "reload_sm_keras = tf.keras.models.load_model(\n",
        "    export_path_sm,\n",
        "    custom_objects={'KerasLayer': hub.KerasLayer}\n",
        ")\n",
        "# Carga el modelo SavedModel desde la carpeta especificada en 'export_path_sm' utilizando 'tf.keras.models.load_model()'.\n",
        "# También se especifica cómo cargar la capa personalizada 'hub.KerasLayer'.\n",
        "\n",
        "reload_sm_keras.summary()\n",
        "# Muestra un resumen del modelo cargado ('reload_sm_keras'), incluyendo la arquitectura de las capas,\n",
        "#el número de parámetros entrenables y no entrenables, y la forma de salida de cada capa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thTbiHE72GL4"
      },
      "source": [
        "Ahora, usemos `reloaded_sm)keras` (modelo Keras recargado de nuestro SavedModel) para hacer predicciones en un lote de imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0oCJrNLKdKj"
      },
      "outputs": [],
      "source": [
        "result_batch = model.predict(image_batch)\n",
        "# Utiliza el modelo original ('model') para realizar predicciones en el lote de imágenes 'image_batch'.\n",
        "\n",
        "reload_sm_keras_result_batch = reload_sm_keras.predict(image_batch)\n",
        "# Utiliza el modelo cargado desde el SavedModel ('reload_sm_keras') para hacer predicciones en el mismo lote de imágenes 'image_batch'.\n",
        "\n",
        "# Estas líneas de código realizan inferencias en el conjunto de imágenes utilizando el modelo original y el modelo cargado desde el SavedModel respectivamente.\n",
        "# Los resultados obtenidos se pueden utilizar para comparar las predicciones entre los dos modelos y verificar la consistencia en sus resultados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUQaxzFj2Q8k"
      },
      "source": [
        "Podemos comprobar que el modelo Keras recargado y el modelo anterior dan el mismo resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJCD9JJxKg9F"
      },
      "outputs": [],
      "source": [
        "(abs(result_batch - reload_sm_keras_result_batch)).max()\n",
        "# Calcula el valor absoluto de la diferencia entre las predicciones del modelo original y las del modelo cargado desde el SavedModel para cada elemento del lote.\n",
        "# Luego, encuentra el valor máximo de estas diferencias.\n",
        "\n",
        "# Esta expresión evalúa la máxima discrepancia entre las predicciones del modelo original y las del modelo cargado desde el SavedModel en el conjunto de imágenes proporcionado.\n",
        "# Un valor cercano a cero indica que las predicciones son prácticamente idénticas entre los dos modelos para las imágenes dadas.\n",
        "# Al comparar estas diferencias máximas, se verifica la consistencia entre las predicciones del modelo original y las del modelo cargado desde el SavedModel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFa6jNc4PW9F"
      },
      "source": [
        "# Parte 8: Descarga tu modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W_y1qMVQeCm"
      },
      "source": [
        "Puede descargar SavedModel a su disco local creando un archivo zip. Usaremos la opción `-r` (recursiva) para comprimir todas las subcarpetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRFoU1xPCGF"
      },
      "outputs": [],
      "source": [
        "!zip -r model.zip {export_path_sm}\n",
        "# Utiliza el comando 'zip' para comprimir y empaquetar los archivos y directorios dentro de la carpeta especificada por 'export_path_sm'.\n",
        "\n",
        "# Al ejecutar este comando en un entorno de línea de comandos, se creará un archivo ZIP llamado 'model.zip' que contiene los archivos y subcarpetas del modelo SavedModel.\n",
        "# Esto facilita la transferencia y el almacenamiento del modelo en una forma compacta y comprimida, ideal para compartirlo o realizar copias de seguridad.\n",
        "# El archivo ZIP resultante se puede utilizar para cargar el modelo SavedModel en otro entorno o sistema sin necesidad de enviar cada archivo individualmente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBZL85RsQBTj"
      },
      "source": [
        "El archivo zip se guarda en el directorio de trabajo actual. Puede ver cuál es el directorio de trabajo actual ejecutando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALP-DfwSQRL8"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "# Utiliza el comando 'ls' para listar los archivos y directorios en el directorio actual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR89aU-SRmsL"
      },
      "source": [
        "Una vez que el archivo esté comprimido, puede descargarlo a su disco local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOXYrlDkNjKQ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('./model.zip')\n",
        "except ImportError:\n",
        "    pass\n",
        "# Intenta descargar el archivo 'model.zip' utilizando la función 'files.download()' de Google Colab.\n",
        "\n",
        "# En el entorno de Google Colab, esta función permite a los usuarios descargar archivos directamente desde el entorno Colab a su sistema local.\n",
        "# Si la importación de 'files' y la función 'download()' tienen éxito, el archivo ZIP del modelo será descargado al sistema local del usuario.\n",
        "# Si se produce un ImportError, lo cual puede suceder fuera del entorno de Google Colab, el código continúa sin hacer nada, ya que la función de descarga no está disponible en ese contexto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzLutxq_SeLQ"
      },
      "source": [
        "El comando `files.download` buscará archivos en su directorio de trabajo actual. Si el archivo que desea descargar está en un directorio distinto al directorio de trabajo actual, debe incluir la ruta al directorio donde se encuentra el archivo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
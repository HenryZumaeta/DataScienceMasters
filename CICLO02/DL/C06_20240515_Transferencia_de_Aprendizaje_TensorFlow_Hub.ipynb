{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryZumaeta/MDS_UNI/blob/Zeta/CICLO02/DL/C06_20240515_Transferencia_de_Aprendizaje_TensorFlow_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxjpzKTvg_dd"
      },
      "source": [
        "# TensorFlow Hub y transferencia de aprendizaje\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crU-iluJIEzw"
      },
      "source": [
        "[TensorFlow Hub](http://tensorflow.org/hub) es un repositorio en línea de modelos de TensorFlow ya entrenados que puedes usar.\n",
        "Estos modelos se pueden utilizar tal cual o se pueden utilizar para el aprendizaje por transferencia.\n",
        "\n",
        "El aprendizaje por transferencia es un proceso en el que se toma un modelo entrenado existente y se lo amplía para realizar trabajo adicional. Esto implica dejar la mayor parte del modelo sin cambios, mientras se agregan y reentrenan las capas finales, para obtener un conjunto diferente de posibles resultados.\n",
        "\n",
        "En este Colab haremos ambas cosas.\n",
        "\n",
        "Aquí puede ver todos los modelos disponibles en [TensorFlow Module Hub](https://tfhub.dev/).\n",
        "\n",
        "## Conceptos que se cubrirán en este Colab\n",
        "\n",
        "1. Utilice un modelo de TensorFlow Hub para la predicción.\n",
        "2. Utilice un modelo de TensorFlow Hub para el conjunto de datos de perros frente a gatos.\n",
        "3. Realice un aprendizaje por transferencia simple con TensorFlow Hub.\n",
        "\n",
        "Antes de iniciar este Colab, debe restablecer el entorno de Colab seleccionando `Tiempo de ejecución -> Restablecer todos los tiempos de ejecución...` en el menú de arriba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RVsYZLEpEWs"
      },
      "source": [
        "# Importar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUCEcRdhnyWn"
      },
      "source": [
        "Algunas importaciones normales que hemos visto antes. El nuevo importa tensorflow_hub que se instaló anteriormente y que este Colab hará un uso intensivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z5hqr0hHtLv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Importa la biblioteca TensorFlow y le asigna el alias 'tf'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZnAHGETHu7e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\n",
        "# Importa la biblioteca matplotlib para visualizaciones y le asigna el alias 'plt'\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# Importa la biblioteca tensorflow_hub para utilizar modelos pre-entrenados desde TensorFlow Hub\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "# Importa la biblioteca tensorflow_datasets para acceder a conjuntos de datos incorporados y funciones de preprocesamiento\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "# Importa el módulo 'layers' de la biblioteca Keras, que es parte de TensorFlow.\n",
        "# Keras para construir y entrenar modelos de aprendizaje profundo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVM2fKGEHIJN"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        " # Importa el módulo 'logging' para habilitar el registro de mensajes durante la ejecución del programa.\n",
        "\n",
        "logger = tf.get_logger()\n",
        "# Obtiene un objeto de registro específico de TensorFlow para registrar mensajes relacionados con TensorFlow.\n",
        "\n",
        "logger.setLevel(logging.ERROR)\n",
        " # Establece el nivel de registro del objeto 'logger' a ERROR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4YuF5HvpM1W"
      },
      "source": [
        "# Parte 1: Utilice TensorFlow Hub MobileNet para realizar predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sh2sPc10V0b"
      },
      "source": [
        "En esta parte de Colab, tomaremos un modelo entrenado, lo cargaremos en Keras y lo probaremos.\n",
        "\n",
        "El modelo que usaremos es MobileNet v2 (pero cualquier modelo de [URL del clasificador de imágenes compatible con tf2 de tfhub.dev](https://tfhub.dev/s?q=tf2&module-type=image-classification) funcionaría). ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEY_Ow5loN6q"
      },
      "source": [
        "## Descargar el clasificador\n",
        "\n",
        "Descargue el modelo MobileNet y cree un modelo Keras a partir de él.\n",
        "MobileNet espera imágenes de 224 $\\times$ 224 píxeles, en 3 canales de color (RGB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_6bGjoPtzau"
      },
      "outputs": [],
      "source": [
        "CLASSIFIER_URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\"\n",
        "IMAGE_RES = 224\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, 3))\n",
        "])  # Crea un modelo secuencial de Keras. Utiliza una capa KerasLayer que carga un modelo preentrenado desde la URL especificada.\n",
        "#La capa espera imágenes de tamaño (IMAGE_RES, IMAGE_RES, 3) como entrada, donde 3 representa los canales de color RGB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQItP1i55-di"
      },
      "source": [
        "MobileNet ha sido entrenado en el conjunto de datos de ImageNet. ImageNet tiene 1000 clases de salida diferentes, y una de ellas son los uniformes militares.\n",
        "Obtengamos una imagen que contenga un uniforme militar que no sea parte de ImageNet y veamos si nuestro modelo puede predecir que es un uniforme militar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5wDjXNjuXGD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Importa la biblioteca NumPy para realizar operaciones numéricas y manipulación de arrays.\n",
        "\n",
        "import PIL.Image as Image\n",
        " # Importa la clase Image del módulo PIL (Python Imaging Library) para trabajar con imágenes en Python.\n",
        "\n",
        "grace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')\n",
        " # Descarga la imagen de Grace Hopper desde la URL especificada y la guarda en un archivo local llamado 'image.jpg'. La función 'get_file' es una utilidad de TensorFlow para descargar archivos.\n",
        "\n",
        "grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES))\n",
        "  # Abre la imagen descargada y la redimensiona a las dimensiones especificadas por IMAGE_RES x IMAGE_RES. Esto es necesario para que coincida con el tamaño de entrada esperado por el modelo.\n",
        "\n",
        "grace_hopper  # Muestra la imagen redimensionada de Grace Hopper. Este paso es comúnmente utilizado para verificar o visualizar los datos antes de procesarlos con modelos de aprendizaje automático.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEmmBnGbLxPp"
      },
      "outputs": [],
      "source": [
        "grace_hopper = np.array(grace_hopper) / 255.0\n",
        " # Normaliza los valores de píxeles de la imagen dividiendo cada valor por 255.0, lo que escala los valores a un rango entre 0 y 1.\n",
        "grace_hopper.shape  # Muestra la forma (shape) del array resultante. En este caso, la forma representa las dimensiones del array, que serán (IMAGE_RES, IMAGE_RES, 3) después de la normalización.\n",
        "# El último valor 3 representa los canales de color (RGB) de la imagen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ic8OEEo2b73"
      },
      "source": [
        "Recuerde, los modelos siempre quieren procesar un lote de imágenes. Entonces, aquí agregamos una dimensión de lote y pasamos la imagen al modelo para su predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMquyn29v8q3"
      },
      "outputs": [],
      "source": [
        "result = model.predict(grace_hopper[np.newaxis, ...])  # Utiliza el modelo para hacer una predicción sobre la imagen\n",
        "#de Grace Hopper después de expandir las dimensiones del array con np.newaxis.\n",
        "# La expansión se realiza para que el modelo pueda aceptar un lote (batch) de imágenes como entrada, y no solo una imagen individual.\n",
        "\n",
        "result.shape  # Muestra la forma (shape) del array resultante después de la predicción.\n",
        "#La forma representa las dimensiones del array de salida del modelo, que generalmente es (1, NUM_CLASSES),\n",
        "#donde NUM_CLASSES es el número de clases de la tarea de clasificación.\n",
        "#En este contexto, el primer valor 1 indica que hay una sola imagen en el lote y el segundo valor representa las probabilidades de pertenencia a cada clase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKzjqENF6jDF"
      },
      "source": [
        "El resultado es un vector de logits de 1001 elementos, que califica la probabilidad de cada clase para la imagen.\n",
        "\n",
        "Entonces, el ID de clase superior se puede encontrar con argmax. Pero, ¿cómo podemos saber qué clase es realmente y, en particular, si esa identificación de clase en el conjunto de datos de ImageNet denota un uniforme militar o algo más?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgXb44vt6goJ"
      },
      "outputs": [],
      "source": [
        "predicted_class = np.argmax(result[0], axis=-1)  # Utiliza la función np.argmax para encontrar el índice\n",
        "#del valor máximo en el array resultante. Esto determina la clase predicha para la imagen.\n",
        "\n",
        "predicted_class  # Muestra la clase predicha para la imagen de Grace Hopper. El valor almacenado\n",
        "#en 'predicted_class' es el índice de la clase con la probabilidad más alta, lo que indica la clase que el modelo predice para la imagen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrxLMajMoxkf"
      },
      "source": [
        "## Decodifica las predicciones\n",
        "\n",
        "Para ver cuál es nuestra predicted_class en el conjunto de datos de ImageNet, descargue las etiquetas de ImageNet y busque la fila que predijo el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij6SrDxcxzry"
      },
      "outputs": [],
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        " # Descarga un archivo de texto que contiene las etiquetas de las clases de ImageNet y lo guarda en 'labels_path'.\n",
        "\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "# Lee el contenido del archivo de etiquetas y crea un array de NumPy con las etiquetas de las clases.\n",
        "\n",
        "plt.imshow(grace_hopper)\n",
        "# Muestra la imagen de Grace Hopper en el gráfico.\n",
        "plt.axis('off')\n",
        "# Desactiva los ejes del gráfico para una mejor presentación visual.\n",
        "\n",
        "predicted_class_name = imagenet_labels[predicted_class]\n",
        "# Obtiene el nombre de la clase predicha a partir del array de etiquetas de ImageNet usando el índice de la clase predicha.\n",
        "\n",
        "_ = plt.title(\"Prediction: \" + predicted_class_name.title())\n",
        " # Establece el título del gráfico, indicando la clase predicha para la imagen de Grace Hopper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6TNYYAM4u2-"
      },
      "source": [
        "Bingo. ¡Nuestro modelo predijo correctamente el uniforme militar!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amfzqn1Oo7Om"
      },
      "source": [
        "# Parte 2: Utilice modelos de TensorFlow Hub para el conjunto de datos de gatos y perros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-nIpVJ94xrw"
      },
      "source": [
        "Ahora usaremos el modelo MobileNet completo y veremos cómo funciona en el conjunto de datos de Perros vs. Gatos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93vvAdGxDMD"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "Podemos usar TensorFlow Datasets para cargar el conjunto de datos Dogs vs Cats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrIUV3V0xDL_"
      },
      "outputs": [],
      "source": [
        "(train_examples, validation_examples), info = tfds.load(\n",
        "    'cats_vs_dogs',  # Carga el conjunto de datos 'cats_vs_dogs' desde TensorFlow Datasets (TFDS).\n",
        "    with_info=True,  # Incluye información adicional sobre el conjunto de datos, como el número de ejemplos y clases.\n",
        "    as_supervised=True,  # Carga los datos en un formato (imagen, etiqueta) para facilitar el entrenamiento supervisado.\n",
        "    split=['train[:80%]', 'train[80%:]'],  # Divide el conjunto de datos en un 80% para entrenamiento y un 20% para validación.\n",
        ")\n",
        "\n",
        "num_examples = info.splits['train'].num_examples  # Obtiene el número total de ejemplos en el conjunto de datos de entrenamiento.\n",
        "num_classes = info.features['label'].num_classes  # Obtiene el número de clases en el conjunto de datos (en este caso, 2 para 'gatos' y 'perros').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlFZ_hwjCLgS"
      },
      "source": [
        "Las imágenes del conjunto de datos Perros vs. Gatos no son todas del mismo tamaño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4lDPkn2cpWZ"
      },
      "outputs": [],
      "source": [
        "for i, example_image in enumerate(train_examples.take(3)):  # Itera sobre los primeros 3 ejemplos del conjunto de datos de entrenamiento.\n",
        "    print(\"Image {} shape: {}\".format(i+1, example_image[0].shape))  # Imprime la forma (shape) de cada imagen del conjunto de datos de entrenamiento.\n",
        "\n",
        "# El bucle `for` recorre los primeros 3 ejemplos del conjunto de datos de entrenamiento (`train_examples`).\n",
        "# Para cada ejemplo, imprime el número de imagen y su forma (shape), indicando las dimensiones de la imagen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbgpD3E6gM2P"
      },
      "source": [
        "Por lo tanto, debemos reformatear todas las imágenes a la resolución esperada por MobileNet (224, 224).\n",
        "\n",
        "Los `.repeat()` y `steps_per_epoch` aquí no son necesarios, pero ahorran ~15 s por época, ya que el shuffle-buffer solo tiene que iniciarse en frío una vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we_ftzQxNf7e"
      },
      "outputs": [],
      "source": [
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / 255.0\n",
        "     # Redimensiona la imagen a las dimensiones especificadas por IMAGE_RES x IMAGE_RES y\n",
        "     #normaliza los valores de píxeles a un rango entre 0 y 1.\n",
        "    return image, label  # Devuelve la imagen formateada y la etiqueta correspondiente.\n",
        "\n",
        "BATCH_SIZE = 32  # Establece el tamaño del lote (batch size) para el entrenamiento y la validación.\n",
        "\n",
        "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        " # Prepara lotes de entrenamiento: mezcla los ejemplos de entrenamiento, aplica el formato de imagen definido,\n",
        " #agrupa los ejemplos en lotes del tamaño especificado y los almacena en memoria para el acceso eficiente durante el entrenamiento.\n",
        "\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        " # Prepara lotes de validación: aplica el formato de imagen definido, agrupa los ejemplos en lotes del\n",
        " #tamaño especificado y los almacena en memoria para su uso durante la validación del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTN7M_GxDLx"
      },
      "source": [
        "## Ejecute el clasificador en un lote de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3fvrZR8xDLv"
      },
      "source": [
        "Recuerde que nuestro objeto `model` sigue siendo el modelo MobileNet completo entrenado en ImageNet, por lo que tiene 1000 clases de salida posibles.\n",
        "ImageNet tiene muchos perros y gatos, así que veamos si puede predecir las imágenes en nuestro conjunto de datos Perros vs. Gatos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kii_jWZYOn0B"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
        " # Obtiene un lote de imágenes y etiquetas del conjunto de datos de entrenamiento utilizando el iterador.\n",
        "image_batch = image_batch.numpy()\n",
        "# Convierte el lote de imágenes a un array de NumPy para su manipulación.\n",
        "label_batch = label_batch.numpy()\n",
        " # Convierte el lote de etiquetas a un array de NumPy para su manipulación.\n",
        "\n",
        "result_batch = model.predict(image_batch)\n",
        "# Utiliza el modelo para hacer predicciones sobre el lote de imágenes y obtiene las probabilidades de pertenencia a cada clase.\n",
        "\n",
        "predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=-1)]\n",
        " # Obtiene los nombres de las clases predichas para el lote de imágenes.\n",
        "predicted_class_names  # Muestra los nombres de las clases predichas para el lote de imágenes.\n",
        "\n",
        "# Los nombres de las clases predichas se almacenan en 'predicted_class_names' para su inspección.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmvSWg9nxDLa"
      },
      "source": [
        "Las etiquetas parecen coincidir con nombres de perros y gatos. Ahora tracemos las imágenes de nuestro conjunto de datos Dogs vs Cats y coloquemos las etiquetas de ImageNet junto a ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXTB22SpxDLP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "# Establece el tamaño de la figura para mostrar las imágenes y las predicciones.\n",
        "\n",
        "for n in range(30):  # Itera sobre los primeros 30 ejemplos del lote de imágenes.\n",
        "    plt.subplot(6, 5, n+1)  # Crea subtramas en una cuadrícula de 6 filas y 5 columnas.\n",
        "    plt.subplots_adjust(hspace=0.3)  # Ajusta el espaciado vertical entre las subtramas.\n",
        "\n",
        "    plt.imshow(image_batch[n])  # Muestra la imagen en la subtrama actual.\n",
        "    plt.title(predicted_class_names[n])  # Establece el título de la subtrama con la clase predicha para la imagen.\n",
        "    plt.axis('off')  # Desactiva los ejes para una mejor presentación visual.\n",
        "\n",
        "_ = plt.suptitle(\"ImageNet predictions\")  # Establece el título principal de la figura.\n",
        "\n",
        "# En este bloque de código, se crea una figura que muestra las primeras\n",
        "# 30 imágenes del lote junto con las clases predichas para cada imagen.\n",
        "# Cada subtrama contiene una imagen y su clase predicha, y\n",
        "#el título principal de la figura indica que son predicciones de ImageNet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzV457OXreQP"
      },
      "source": [
        "# Parte 3: Realice un aprendizaje por transferencia simple con TensorFlow Hub\n",
        "\n",
        "Ahora usemos TensorFlow Hub para realizar Transfer Learning.\n",
        "\n",
        "Con el aprendizaje por transferencia reutilizamos partes de un modelo ya entrenado y cambiamos la capa final, o varias capas, del modelo, y luego volvemos a entrenar esas capas en nuestro propio conjunto de datos.\n",
        "\n",
        "Además de los modelos completos, TensorFlow Hub también distribuye modelos sin la última capa de clasificación. Estos se pueden utilizar para transferir fácilmente el aprendizaje. Continuaremos usando MobileNet v2 porque en partes posteriores de este curso tomaremos este modelo y lo implementaremos en un dispositivo móvil usando [TensorFlow Lite] (https://www.tensorflow.org/lite). Cualquier [URL de vector de característica de imagen de tfhub.dev](https://tfhub.dev/s?module-type=image-feature-vector&q=tf2) funcionaría aquí.\n",
        "\n",
        "También continuaremos usando el conjunto de datos Perros vs Gatos, por lo que podremos comparar el rendimiento de este modelo con los que creamos desde cero anteriormente.\n",
        "\n",
        "Tenga en cuenta que llamamos al modelo parcial de TensorFlow Hub (sin la capa de clasificación final) \"feature_extractor\". El razonamiento de este término es que llevará la entrada hasta una capa que contiene varias características. Por lo tanto, ha realizado la mayor parte del trabajo para identificar el contenido de una imagen, excepto crear la distribución de probabilidad final. Es decir, ha extraído las características de la imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wB030nezBwI"
      },
      "outputs": [],
      "source": [
        "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
        " # Especifica la URL del modelo de extracción de características de MobileNetV2 en TensorFlow Hub.\n",
        "\n",
        "feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, 3))\n",
        "# Crea una capa de extracción de características utilizando el modelo preentrenado de MobileNetV2 desde la URL especificada.\n",
        "# La capa está configurada para aceptar imágenes de tamaño (IMAGE_RES, IMAGE_RES, 3) como entrada, donde 3 representa los canales de color RGB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkSvAPvKOWg2"
      },
      "source": [
        "Pasemos un lote de imágenes a través de esto y veamos la forma final. 32 es el número de imágenes y 1280 es el número de neuronas en la última capa del modelo parcial de TensorFlow Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of7i-35F09ls"
      },
      "outputs": [],
      "source": [
        "feature_batch = feature_extractor(image_batch)  # Utiliza la capa de extracción de características para procesar el lote de imágenes.\n",
        "\n",
        "print(feature_batch.shape)  # Imprime la forma (shape) del array resultante después de la extracción de características.\n",
        "\n",
        "# En este bloque de código, se utiliza la capa de extracción de características para procesar el lote de imágenes del conjunto de datos de entrenamiento.\n",
        "# La forma del array resultante se imprime para mostrar las dimensiones de las características extraídas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFmF7A5E4tk"
      },
      "source": [
        "Congele las variables en la capa del extractor de características, de modo que el entrenamiento solo modifique la capa del clasificador final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg5ar6rcE4H-"
      },
      "outputs": [],
      "source": [
        "feature_extractor.trainable = False\n",
        "# Establece el atributo 'trainable' de la capa de extracción de características en False.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPVeouTksO9q"
      },
      "source": [
        "## Adjunte un encabezado de clasificación\n",
        "\n",
        "Ahora envuelva la capa central en un modelo `tf.keras.Sequential` y agregue una nueva capa de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGcY27fY1q3Q"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,  # Agrega la capa de extracción de características (MobileNetV2) al modelo.\n",
        "    layers.Dense(2)  # Agrega una capa densa con 2 unidades para la clasificación binaria de 'gatos' y 'perros'.\n",
        "])\n",
        "\n",
        "model.summary()\n",
        " # Muestra un resumen del modelo, incluyendo la arquitectura de las capas y el número de parámetros entrenables.\n",
        "\n",
        "# En este bloque de código, se crea un modelo secuencial de Keras que consiste en la capa de extracción de características (\n",
        "    #MobileNetV2) seguida de una capa densa con 2 unidades para la clasificación binaria de 'gatos' y 'perros'.\n",
        "# El método 'summary()' muestra una descripción detallada de la arquitectura del modelo y el número de parámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHbXQqIquFxQ"
      },
      "source": [
        "## Entrena el modelo\n",
        "\n",
        "Ahora entrenamos este modelo como cualquier otro, llamando primero a `compile` y luego a `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n0Wb9ylKd8R"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',  # Configura el optimizador Adam para el entrenamiento del modelo.\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # Utiliza la entropía cruzada categórica como función de pérdida para problemas de clasificación.\n",
        "    metrics=['accuracy']  # Utiliza la Exactitud como métrica para evaluar el rendimiento del modelo durante el entrenamiento.\n",
        ")\n",
        "\n",
        "EPOCHS = 5  # Establece el número de épocas para entrenar el modelo.\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,  # Utiliza el lote de entrenamiento para el entrenamiento del modelo.\n",
        "    epochs=EPOCHS,  # Número de épocas para entrenar el modelo.\n",
        "    validation_data=validation_batches  # Utiliza el lote de validación para evaluar el modelo después de cada época de entrenamiento.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76as-K8-vFQJ"
      },
      "source": [
        "Puede ver que obtenemos una precisión de validación de ~99%, lo cual es absolutamente asombroso. Esta es una gran mejora con respecto al modelo que creamos en la lección anterior, donde pudimos obtener ~99% de precisión. La razón de esta diferencia es que MobileNet fue diseñado cuidadosamente durante mucho tiempo por expertos y luego entrenado en un conjunto de datos masivo (ImageNet).\n",
        "\n",
        "Aunque no es equivalente a TensorFlow Hub, puede consultar cómo crear MobileNet en Keras [aquí](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py).\n",
        "\n",
        "Tracemos los gráficos de precisión/pérdida de entrenamiento y validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d28dhbFpr98b"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']  # Obtiene la precisión del entrenamiento en cada época.\n",
        "val_acc = history.history['val_accuracy']  # Obtiene la precisión de la validación en cada época.\n",
        "\n",
        "loss = history.history['loss']  # Obtiene la pérdida del entrenamiento en cada época.\n",
        "val_loss = history.history['val_loss']  # Obtiene la pérdida de la validación en cada época.\n",
        "\n",
        "epochs_range = range(EPOCHS)  # Crea un rango de épocas para utilizar en el gráfico.\n",
        "\n",
        "plt.figure(figsize=(8, 8))  # Establece el tamaño de la figura para mostrar las gráficas de precisión y pérdida.\n",
        "\n",
        "# Gráfico de precisión del entrenamiento y validación.\n",
        "plt.subplot(1, 2, 1)  # Crea una subtrama en la primera posición de una cuadrícula de 1 fila y 2 columnas.\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')  # Dibuja la precisión del entrenamiento.\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')  # Dibuja la precisión de la validación.\n",
        "plt.legend(loc='lower right')  # Muestra la leyenda en la esquina inferior derecha.\n",
        "plt.title('Training and Validation Accuracy')  # Establece el título del gráfico.\n",
        "\n",
        "# Gráfico de pérdida del entrenamiento y validación.\n",
        "plt.subplot(1, 2, 2)  # Crea una subtrama en la segunda posición de la cuadrícula.\n",
        "plt.plot(epochs_range, loss, label='Training Loss')  # Dibuja la pérdida del entrenamiento.\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')  # Dibuja la pérdida de la validación.\n",
        "plt.legend(loc='upper right')  # Muestra la leyenda en la esquina superior derecha.\n",
        "plt.title('Training and Validation Loss')  # Establece el título del gráfico.\n",
        "\n",
        "plt.show()  # Muestra los gráficos de precisión y pérdida.\n",
        "\n",
        "# En este bloque de código, se generan y muestran gráficas que representan la precisión y pérdida del modelo durante el entrenamiento y la validación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmoDisGvNye"
      },
      "source": [
        "Lo que resulta un poco curioso aquí es que el rendimiento de la validación es mejor que el rendimiento del entrenamiento, desde el principio hasta el final de la ejecución.\n",
        "\n",
        "Una razón para esto es que el rendimiento de la validación se mide al final de la época, pero el rendimiento del entrenamiento son los valores promedio a lo largo de la época.\n",
        "\n",
        "Sin embargo, la razón más importante es que estamos reutilizando una gran parte de MobileNet que ya está entrenado en imágenes de perros y gatos. Mientras se realiza el entrenamiento, la red sigue realizando un aumento de imágenes en las imágenes de entrenamiento, pero no en el conjunto de datos de validación. Esto significa que las imágenes de entrenamiento pueden ser más difíciles de clasificar en comparación con las imágenes normales del conjunto de datos de validación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb__ZN8uFn-D"
      },
      "source": [
        "## Consulta las predicciones\n",
        "\n",
        "Para rehacer la trama anterior, primero obtenga la lista ordenada de nombres de clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_Zvg2i0fzJu"
      },
      "outputs": [],
      "source": [
        "class_names = np.array(info.features['label'].names)\n",
        "class_names\n",
        "# Obtiene y muestra las etiquetas de clase del conjunto de datos. En este caso,\n",
        "#las etiquetas son 'gato' y 'perro', y se almacenan en el array 'class_names'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Olg6MsNGJTL"
      },
      "source": [
        "Ejecute el lote de imágenes a través del modelo y convierta los índices en nombres de clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCLVCpEjJ_VP"
      },
      "outputs": [],
      "source": [
        "predicted_batch = model.predict(image_batch)\n",
        " # Realiza predicciones sobre el lote de imágenes utilizando el modelo entrenado.\n",
        "predicted_batch = tf.squeeze(predicted_batch).numpy()\n",
        "# Elimina las dimensiones adicionales del array resultante y lo convierte a un array de NumPy.\n",
        "predicted_ids = np.argmax(predicted_batch, axis=-1)\n",
        "# Obtiene los índices de las clases predichas para cada imagen.\n",
        "predicted_class_names = class_names[predicted_ids]\n",
        " # Obtiene los nombres de las clases predichas a partir de los índices utilizando las etiquetas del conjunto de datos.\n",
        "\n",
        "predicted_class_names\n",
        "# Muestra los nombres de las clases predichas para el lote de imágenes del conjunto de datos de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGbZxl9GZs-"
      },
      "source": [
        "Veamos las etiquetas verdaderas y las previstas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9IhOmGI5dJ"
      },
      "outputs": [],
      "source": [
        "print(\"Labels: \", label_batch)\n",
        "print(\"Predicted labels: \", predicted_ids)\n",
        "# Imprime las etiquetas reales (ground truth) del lote de imágenes (label_batch) y\n",
        "# las etiquetas predichas por el modelo (predicted_ids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC_AYRJU9NQe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,9))  # Establece el tamaño de la figura para mostrar las imágenes y las predicciones.\n",
        "\n",
        "for n in range(30):  # Itera sobre los primeros 30 ejemplos del lote de imágenes.\n",
        "    plt.subplot(6, 5, n+1)  # Crea subtramas en una cuadrícula de 6 filas y 5 columnas.\n",
        "    plt.subplots_adjust(hspace=0.3)  # Ajusta el espaciado vertical entre las subtramas.\n",
        "\n",
        "    plt.imshow(image_batch[n])  # Muestra la imagen en la subtrama actual.\n",
        "    color = \"blue\" if predicted_ids[n] == label_batch[n] else \"red\"  # Determina el color del título según si la predicción es correcta o incorrecta.\n",
        "    plt.title(predicted_class_names[n].title(), color=color)  # Establece el título de la subtrama con la clase predicha para la imagen, resaltando las predicciones incorrectas en rojo.\n",
        "    plt.axis('off')  # Desactiva los ejes para una mejor presentación visual.\n",
        "\n",
        "_ = plt.suptitle(\"Model predictions (blue: correct, red: incorrect)\")  # Establece el título principal de la figura para indicar las predicciones del modelo y resaltar las predicciones incorrectas en rojo.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR0JCowQoQww"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}